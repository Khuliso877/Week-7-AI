Policy Guideline: Ethical Deployment
and Use of Artificial Intelligence (AI) in
Healthcare
Effective Date: January 1, 2026 Scope: All personnel, systems, and vendors involved in
developing, deploying, or utilizing AI/ML models for clinical, operational, or administrative
purposes.
1. Purpose and Principles
AI must uphold the core ethical principles of Beneficence, Non-Maleficence, Autonomy, and
Justice, ensuring patient care, preserving trust, and promoting equity.
2. Patient Consent Protocols and Autonomy
AI utilization must always respect patient autonomy.
2.1 Dynamic, Context-Specific Consent
1. Informed Consent: Patients must be explicitly informed when AI is used for data
analysis or to recommend treatment.
2. Dynamic Consent: Consent must be specific to the AI application's context (e.g.,
triage vs. optimization). Broad, one-time consent for all future AI uses is prohibited.
3. Right to Withdraw/Opt-Out: Patients retain the right to withdraw their personal data
from future AI model retraining at any time. Clinicians must always retain the
authority to override AI recommendations ("human in the loop").
3. Bias Mitigation and Health Equity
AI systems must not perpetuate or amplify existing health disparities.
3.1 Data Audit and Fair Representation
1. Mandatory Bias Audits: Before deployment, conduct rigorous auditing to assess AI
performance across diverse demographic subgroups (race, ethnicity, age, and
socioeconomic status).
2. Data Quality and Provenance: Documentation must verify that training data is
representative of the target population and free from systematic biases leading to
disparate clinical outcomes.
3. Fairness Metrics: Development teams must define and track pre-specified fairness
metrics (e.g., Equal Opportunity Difference) to ensure equitable model performance
across high-risk patient groups.
3.2 Post-Deployment Monitoring and Remediation
1. Adverse Outcome Tracking: Implement systems for real-time monitoring of AI
outputs to detect bias drift or emergent discriminatory results.
2. Remediation: If unfair performance is detected, the AI system must be immediately
flagged for re-calibration or temporary removal from clinical use until the issue is fully
resolved.
4. Transparency and Explainability
All AI systems must be understandable to clinicians and, where possible, explainable to
patients.
4.1 Required Documentation
1. Model Cards: Provide a publicly accessible "Model Card" for every deployed AI
model. This must detail its intended use, dataset composition, key performance and
fairness metrics, and known failure modes.
2. Black Box Justification: Systems lacking inherent interpretability must provide an
additional layer of local explanation (e.g., LIME or SHAP values) to justify specific
clinical recommendations.
4.2 Accountability and Audit
1. Clear Ownership: A designated human clinical lead and a technical owner must be
assigned to every deployed AI system, jointly responsible for compliance and
addressing failures.
2. Logging and Audit Trails: All AI recommendations, human override decisions, and
associated patient data must be securely logged to maintain a complete, auditable
trail for post-incident review and regulatory compliance
